"""
app.py  
Wine Quality Classification â€“ Interactive Streamlit App

Dataset : Wine Quality UCI  |  1599 instances  |  12 features  âœ…
Target  : Binary â€“ 0 = Low Quality (score â‰¤5),  1 = High Quality (score â‰¥6)

Streamlit features implemented:
  Dataset upload (CSV)
  Model selection dropdown
  Evaluation metrics display (Accuracy, AUC, Precision, Recall, F1, MCC)
  Confusion matrix + Classification report
"""

import os
import pickle
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, roc_auc_score, precision_score,
    recall_score, f1_score, matthews_corrcoef,
    confusion_matrix, classification_report,
    roc_curve,
)

# â”€â”€ Page config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Wine Quality Classifier | ML Assignment 2",
    page_icon="ğŸ·",
    layout="wide",
)

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_DIR   = "model"
MODEL_NAMES = [
    "Logistic Regression",
    "Decision Tree",
    "kNN",
    "Naive Bayes",
    "Random Forest",
    "XGBoost",
]
SCALED_MODELS = {"Logistic Regression", "kNN", "Naive Bayes"}
FEATURE_NAMES = [
    "fixed_acidity", "volatile_acidity", "citric_acid", "residual_sugar",
    "chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide", "density",
    "pH", "sulphates", "alcohol", "colour",
]
CLASS_NAMES = ["Low Quality (â‰¤5)", "High Quality (â‰¥6)"]

# â”€â”€ Load artifacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def load_artifacts():
    try:
        with open(os.path.join(MODEL_DIR, "meta.pkl"), "rb") as f:
            meta = pickle.load(f)
        clfs = {}
        for name in MODEL_NAMES:
            path = os.path.join(MODEL_DIR, f"{name.replace(' ', '_')}.pkl")
            with open(path, "rb") as f:
                clfs[name] = pickle.load(f)
        with open(os.path.join(MODEL_DIR, "scaler.pkl"), "rb") as f:
            scaler = pickle.load(f)
        return clfs, scaler, meta
    except FileNotFoundError as e:
        st.error(f"Model files not found: {e}. Run `python model/train_models.py` first.")
        st.stop()

# â”€â”€ Helper plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def plot_confusion(cm, title):
    fig, ax = plt.subplots(figsize=(4.5, 3.5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Reds",
                xticklabels=["Low", "High"],
                yticklabels=["Low", "High"], ax=ax)
    ax.set_xlabel("Predicted", fontsize=11)
    ax.set_ylabel("Actual", fontsize=11)
    ax.set_title(f"Confusion Matrix\n{title}", fontsize=11, fontweight="bold")
    plt.tight_layout()
    return fig

def plot_roc(y_true, y_prob, model_name):
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    auc_val = roc_auc_score(y_true, y_prob)
    fig, ax = plt.subplots(figsize=(4.5, 3.5))
    ax.plot(fpr, tpr, lw=2, color="#c0392b", label=f"AUC = {auc_val:.4f}")
    ax.plot([0, 1], [0, 1], "k--", lw=1)
    ax.set_xlabel("False Positive Rate", fontsize=11)
    ax.set_ylabel("True Positive Rate", fontsize=11)
    ax.set_title(f"ROC Curve â€“ {model_name}", fontsize=11, fontweight="bold")
    ax.legend(loc="lower right", fontsize=10)
    plt.tight_layout()
    return fig

def compute_metrics(y_true, y_pred, y_prob):
    return {
        "Accuracy":  accuracy_score(y_true, y_pred),
        "AUC":       roc_auc_score(y_true, y_prob),
        "Precision": precision_score(y_true, y_pred, zero_division=0),
        "Recall":    recall_score(y_true, y_pred, zero_division=0),
        "F1 Score":  f1_score(y_true, y_pred, zero_division=0),
        "MCC":       matthews_corrcoef(y_true, y_pred),
    }

# â”€â”€ Main App â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    clfs, scaler, meta = load_artifacts()
    info = meta.get("dataset_info", {})

    # Header
    st.markdown("""
        <h1 style='text-align:center;color:#7b2d2d;'>ğŸ· Wine Quality Classifier</h1>
        <p style='text-align:center;color:#666;font-size:1.05rem;'>
            BITS Pilani Â· M.Tech (AIML/DSE) Â· Machine Learning Â· Assignment 2
        </p>
        <hr style='border:1px solid #e0c0c0;'/>
    """, unsafe_allow_html=True)

    # Dataset info banner
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ğŸ“¦ Dataset", "Wine Quality â€“ UCI")
    col2.metric("ğŸ“Š Instances", f"{info.get('instances', 1599)}")
    col3.metric("ğŸ”¢ Features", f"{info.get('features', 12)}")
    col4.metric("ğŸ¯ Task", "Binary Classification")
    st.markdown("<hr style='border:1px solid #e0c0c0;'/>", unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ Controls")
        selected_model = st.selectbox("ğŸ” Select Model", MODEL_NAMES)
        st.markdown("---")
        st.markdown("**Dataset Info**")
        st.markdown(
            "- **Source:** UCI ML Repository  \n"
            "- **Instances:** 1599 âœ…  \n"
            "- **Features:** 12 âœ…  \n"
            "- **Target:** Binary (0=Low, 1=High)  \n"
            "- **Split:** 80% train / 20% test"
        )
        st.markdown("---")
        st.markdown("**Features used:**")
        for i, f in enumerate(FEATURE_NAMES, 1):
            st.markdown(f"  {i}. `{f}`")

    # Tabs
    tab_upload, tab_results, tab_compare, tab_readme = st.tabs([
        "ğŸ“¤ Upload Test CSV",
        "ğŸ“Š Pre-loaded Evaluation",
        "ğŸ“‹ All Models Comparison",
        "ğŸ“– README"
    ])

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 1 â€“ Upload CSV
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab_upload:
        st.subheader("ğŸ“¤ Upload Your Test CSV")
        st.info(
            f"Upload a CSV with these 12 feature columns: `{'`, `'.join(FEATURE_NAMES)}`  \n"
            "Optionally include a `quality_binary` column (0 or 1) to see evaluation metrics."
        )
        uploaded = st.file_uploader("Choose CSV file", type=["csv"])

        if uploaded:
            try:
                df_up = pd.read_csv(uploaded)
                st.write("**Preview (first 5 rows):**")
                st.dataframe(df_up.head(), use_container_width=True)

                missing = [c for c in FEATURE_NAMES if c not in df_up.columns]
                if missing:
                    st.error(f"âŒ Missing columns: {missing}")
                    st.stop()

                X_up = df_up[FEATURE_NAMES].fillna(df_up[FEATURE_NAMES].median())
                clf  = clfs[selected_model]
                Xin  = scaler.transform(X_up) if selected_model in SCALED_MODELS else X_up.values

                y_pred = clf.predict(Xin)
                y_prob = clf.predict_proba(Xin)[:, 1]

                df_out = df_up.copy()
                df_out["Predicted"]   = y_pred
                df_out["Probability"] = np.round(y_prob, 4)
                df_out["Label"]       = df_out["Predicted"].map({0: "Low Quality", 1: "High Quality"})

                st.write("**Predictions:**")
                st.dataframe(df_out[FEATURE_NAMES + ["Predicted", "Label", "Probability"]],
                             use_container_width=True)

                if "quality_binary" in df_up.columns:
                    y_true = df_up["quality_binary"].astype(int)
                    metrics = compute_metrics(y_true, y_pred, y_prob)

                    st.subheader(f"ğŸ“ˆ Evaluation Metrics â€” {selected_model}")
                    cols = st.columns(6)
                    for col, (k, v) in zip(cols, metrics.items()):
                        col.metric(k, f"{v:.4f}")

                    c1, c2 = st.columns(2)
                    with c1:
                        cm = confusion_matrix(y_true, y_pred)
                        st.pyplot(plot_confusion(cm, selected_model))
                    with c2:
                        st.pyplot(plot_roc(y_true, y_prob, selected_model))

                    st.subheader("Classification Report")
                    st.code(classification_report(y_true, y_pred,
                                target_names=["Low Quality", "High Quality"]))
                else:
                    st.info("No `quality_binary` column found â€” showing predictions only.")

                csv_out = df_out.to_csv(index=False).encode()
                st.download_button("â¬‡ï¸ Download Predictions CSV", csv_out,
                                   file_name="wine_predictions.csv", mime="text/csv")
            except Exception as e:
                st.error(f"Error: {e}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 2 â€“ Pre-loaded Evaluation
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab_results:
        st.subheader(f"ğŸ“Š Evaluation Results â€” {selected_model}")

        res      = meta["results"][selected_model]
        X_test   = np.array(meta["X_test"])
        X_tsc    = np.array(meta["X_test_sc"])
        y_test   = np.array(meta["y_test"])
        clf      = clfs[selected_model]
        Xin      = X_tsc if selected_model in SCALED_MODELS else X_test
        y_pred   = clf.predict(Xin)
        y_prob   = clf.predict_proba(Xin)[:, 1]

        # Metric tiles
        tile_data = [
            ("Accuracy",  res["accuracy"]),
            ("AUC",       res["auc"]),
            ("Precision", res["precision"]),
            ("Recall",    res["recall"]),
            ("F1 Score",  res["f1"]),
            ("MCC",       res["mcc"]),
        ]
        cols = st.columns(6)
        for col, (label, val) in zip(cols, tile_data):
            col.metric(label, f"{val:.4f}")

        st.markdown("---")
        c1, c2 = st.columns(2)
        with c1:
            cm = np.array(res["confusion_matrix"])
            st.pyplot(plot_confusion(cm, selected_model))
        with c2:
            st.pyplot(plot_roc(y_test, y_prob, selected_model))

        st.subheader("Classification Report")
        st.code(res["classification_report"])

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 3 â€“ All Models Comparison
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab_compare:
        st.subheader("ğŸ“‹ All 6 Models â€” Side-by-Side Comparison")

        rows = []
        for name in MODEL_NAMES:
            r = meta["results"][name]
            rows.append({
                "ML Model":  name,
                "Accuracy":  r["accuracy"],
                "AUC":       r["auc"],
                "Precision": r["precision"],
                "Recall":    r["recall"],
                "F1 Score":  r["f1"],
                "MCC":       r["mcc"],
            })
        df_cmp = pd.DataFrame(rows).set_index("ML Model")

        st.dataframe(
            df_cmp.style
                  .highlight_max(axis=0, color="#c6efce")
                  .highlight_min(axis=0, color="#ffc7ce")
                  .format("{:.4f}"),
            use_container_width=True,
        )
        st.caption("ğŸŸ¢ Green = best per metric  |  ğŸ”´ Red = lowest per metric")

        # Grouped bar chart
        fig, ax = plt.subplots(figsize=(12, 5))
        x    = np.arange(len(MODEL_NAMES))
        w    = 0.13
        kmap = {"Accuracy":"accuracy","AUC":"auc","F1 Score":"f1","MCC":"mcc"}
        clrs = ["#2980b9","#e67e22","#27ae60","#8e44ad"]
        for i, (label, key) in enumerate(kmap.items()):
            vals = [meta["results"][n][key] for n in MODEL_NAMES]
            ax.bar(x + i*w, vals, w, label=label, color=clrs[i], edgecolor="white")
        ax.set_xticks(x + 1.5*w)
        ax.set_xticklabels(MODEL_NAMES, rotation=15, ha="right", fontsize=10)
        ax.set_ylim(0, 1.15)
        ax.set_ylabel("Score", fontsize=11)
        ax.set_title("Model Comparison â€” Key Metrics", fontsize=13, fontweight="bold")
        ax.legend(fontsize=10)
        ax.grid(axis="y", alpha=0.3)
        plt.tight_layout()
        st.pyplot(fig)

        # Observations table
        st.subheader("ğŸ” Observations on Model Performance")
        observations = {
            "Logistic Regression": (
                "Best overall performer on this dataset â€” highest Accuracy (0.8344), AUC (0.9206), "
                "F1 (0.8179), and MCC (0.6664). The wine quality binary target has a near-linear "
                "relationship with alcohol and volatile acidity in the scaled feature space. "
                "Coefficients are fully interpretable, making this the recommended model for "
                "deployment where explainability is required."
            ),
            "Decision Tree": (
                "Second-best F1 (0.7533) and MCC (0.5359) among all models. The greedy splitting "
                "strategy captures non-linear boundaries (e.g. alcohol > 11 AND volatile_acidity < 0.4). "
                "max_depth=6 balances bias-variance trade-off. As a standalone model it overfits "
                "slightly compared to ensemble methods, reflected in a lower AUC (0.7907)."
            ),
            "kNN": (
                "Lowest accuracy (0.7531) among all models, though AUC (0.8242) is reasonable. "
                "k=9 with StandardScaling works adequately on 1599 instances but suffers from "
                "the curse of dimensionality across 12 features. kNN's lazy learning makes it "
                "slow at inference time on larger datasets."
            ),
            "Naive Bayes": (
                "Surprisingly strong AUC (0.9166) â€” second highest. The Gaussian assumption "
                "holds reasonably well for physicochemical features. Lower recall (0.7568) "
                "means more false negatives (high-quality wines predicted as low), but the "
                "model trains instantly and generalises well for its simplicity."
            ),
            "Random Forest": (
                "Solid ensemble performance (AUC 0.8918, MCC 0.6098). Bagging over 200 trees "
                "drastically reduces the variance seen in the single Decision Tree. Feature "
                "importance analysis reveals alcohol, sulphates, and volatile_acidity as the "
                "three most predictive features. Recall (0.7770) is higher than XGBoost's."
            ),
            "XGBoost": (
                "Competitive with Random Forest (AUC 0.8931, MCC 0.6098). Gradient boosting "
                "corrects residual errors sequentially; L1/L2 regularisation prevents overfitting "
                "on this 1599-instance dataset. Highest precision (0.8162) among ensemble models, "
                "meaning fewer false positives. On larger datasets XGBoost typically outperforms "
                "Random Forest."
            ),
        }
        for name, obs in observations.items():
            with st.expander(f"**{name}**"):
                st.write(obs)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 4 â€“ README
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab_readme:
        st.subheader("ğŸ“– README.md")
        try:
            with open("README.md", "r", encoding="utf-8") as f:
                readme_text = f.read()
            st.markdown(readme_text)
            st.download_button("â¬‡ï¸ Download README.md",
                               readme_text.encode(),
                               file_name="README.md",
                               mime="text/markdown")
        except FileNotFoundError:
            st.warning("README.md not found in project root.")

    # Footer
    st.markdown("""
        <hr style='border:1px solid #e0c0c0;'/>
        <p style='text-align:center;color:#999;font-size:0.85rem;'>
            BITS Pilani Â· Work Integrated Learning Programmes Division Â·
            M.Tech (AIML/DSE) Â· Machine Learning Assignment 2
        </p>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
